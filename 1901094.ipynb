{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#解压数据\r\n",
    "!unzip -o data/data86770/seg.zip -d /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PaddleSeg'...\n",
      "remote: Enumerating objects: 11141, done.\u001b[K\n",
      "remote: Counting objects: 100% (11141/11141), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5537/5537), done.\u001b[K\n",
      "remote: Total 11141 (delta 7556), reused 8285 (delta 5439), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (11141/11141), 157.00 MiB | 9.81 MiB/s, done.\n",
      "Resolving deltas: 100% (7556/7556), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "#已经克隆了，不用再克隆\r\n",
    "# !git clone https://gitee.com/PaddlePaddle/PaddleSeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 生成train.txt 和val.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\r\n",
    "import os\r\n",
    "random.seed(2020)\r\n",
    "mask_dir  = '/home/aistudio/work/seg/Train/masks'\r\n",
    "img_dir = '/home/aistudio/work/seg/Train/fundus_image'\r\n",
    "path_list = list()\r\n",
    "for img in os.listdir(img_dir):\r\n",
    "    img_path = os.path.join(img_dir,img)\r\n",
    "    mask_path = os.path.join(mask_dir,img.replace('jpg', 'png'))\r\n",
    "    path_list.append((img_path, mask_path))\r\n",
    "random.shuffle(path_list)\r\n",
    "ratio = 0.8\r\n",
    "train_f = open('/home/aistudio/work/seg/Train/train.txt','w') \r\n",
    "val_f = open('/home/aistudio/work/seg/Train/val.txt' ,'w')\r\n",
    "\r\n",
    "for i ,content in enumerate(path_list):\r\n",
    "    img, mask = content\r\n",
    "    text = img + ' ' + mask + '\\n'\r\n",
    "    if i < len(path_list) * ratio:\r\n",
    "        train_f.write(text)\r\n",
    "    else:\r\n",
    "        val_f.write(text)\r\n",
    "train_f.close()\r\n",
    "val_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 配置文件如下\n",
    "```\n",
    "batch_size: 4\n",
    "iters: 16000\n",
    "\n",
    "train_dataset:\n",
    "  type: Dataset\n",
    "  dataset_root: /home/aistudio/work/seg/Train/\n",
    "  train_path: /home/aistudio/work/seg/Train/train.txt\n",
    "  num_classes: 2\n",
    "  transforms:\n",
    "    - type: Resize\n",
    "      target_size: [512, 512]\n",
    "    # - type: RandomRotation\n",
    "    #   max_rotation: 15\n",
    "    - type: RandomHorizontalFlip\n",
    "    - type: RandomDistort\n",
    "      brightness_range: 0.4\n",
    "      contrast_range: 0.4\n",
    "      saturation_range: 0.4\n",
    "    - type: Normalize\n",
    "  mode: train\n",
    "\n",
    "val_dataset:\n",
    "  type: Dataset\n",
    "  dataset_root: /home/aistudio/work/seg/Train/\n",
    "  val_path: /home/aistudio/work/seg/Train/val.txt\n",
    "  num_classes: 2\n",
    "  transforms:\n",
    "    - type: Resize\n",
    "      target_size: [512, 512]\n",
    "    - type: Normalize\n",
    "  mode: val\n",
    "\n",
    "\n",
    "optimizer:\n",
    "  type: sgd\n",
    "  momentum: 0.9\n",
    "  weight_decay: 4.0e-5\n",
    "\n",
    "learning_rate:\n",
    "  value: 0.00125\n",
    "  decay:\n",
    "    type: poly\n",
    "    power: 0.9\n",
    "    end_lr: 0.0\n",
    "\n",
    "loss:\n",
    "  types:\n",
    "    - type: MixedLoss\n",
    "      losses:\n",
    "        - type: CrossEntropyLoss\n",
    "        - type: DiceLoss\n",
    "      coef: [0.7, 0.3]\n",
    "  coef: [1]\n",
    "\n",
    "model:\n",
    "  type: UNet\n",
    "  num_classes: 2\n",
    "  use_deconv: False\n",
    "  pretrained: /home/aistudio/unetmodel.pdparams\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/PaddleSeg\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n",
      "2021-05-06 21:35:50 [INFO]\t\n",
      "------------Environment Information-------------\n",
      "platform: Linux-4.13.0-36-generic-x86_64-with-debian-stretch-sid\n",
      "Python: 3.7.4 (default, Aug 13 2019, 20:35:49) [GCC 7.3.0]\n",
      "Paddle compiled with cuda: True\n",
      "NVCC: Cuda compilation tools, release 10.1, V10.1.243\n",
      "cudnn: 7.6\n",
      "GPUs used: 1\n",
      "CUDA_VISIBLE_DEVICES: None\n",
      "GPU: ['GPU 0: Tesla V100-SXM2-16GB']\n",
      "GCC: gcc (Ubuntu 7.5.0-3ubuntu1~16.04) 7.5.0\n",
      "PaddlePaddle: 2.0.2\n",
      "OpenCV: 4.1.1\n",
      "------------------------------------------------\n",
      "2021-05-06 21:35:50 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "batch_size: 4\n",
      "iters: 16000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.0\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.00125\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  types:\n",
      "  - coef:\n",
      "    - 0.7\n",
      "    - 0.3\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: DiceLoss\n",
      "    type: MixedLoss\n",
      "model:\n",
      "  num_classes: 2\n",
      "  pretrained: /home/aistudio/unetmodel.pdparams\n",
      "  type: UNet\n",
      "  use_deconv: false\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 4.0e-05\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/work/seg/Train/\n",
      "  mode: train\n",
      "  num_classes: 2\n",
      "  train_path: /home/aistudio/work/seg/Train/train.txt\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 512\n",
      "    - 512\n",
      "    type: Resize\n",
      "  - type: RandomHorizontalFlip\n",
      "  - brightness_range: 0.4\n",
      "    contrast_range: 0.4\n",
      "    saturation_range: 0.4\n",
      "    type: RandomDistort\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/work/seg/Train/\n",
      "  mode: val\n",
      "  num_classes: 2\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 512\n",
      "    - 512\n",
      "    type: Resize\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "  val_path: /home/aistudio/work/seg/Train/val.txt\n",
      "------------------------------------------------\n",
      "W0506 21:35:50.598582 10081 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W0506 21:35:50.598624 10081 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n",
      "2021-05-06 21:35:53 [INFO]\tLoading pretrained model from /home/aistudio/unetmodel.pdparams\n",
      "2021-05-06 21:35:53 [WARNING]\t[SKIP] Shape of pretrained params cls.weight doesn't match.(Pretrained: (19, 64, 3, 3), Actual: [2, 64, 3, 3])\n",
      "2021-05-06 21:35:53 [WARNING]\t[SKIP] Shape of pretrained params cls.bias doesn't match.(Pretrained: (19,), Actual: [2])\n",
      "2021-05-06 21:35:53 [WARNING]\t[SKIP] Shape of pretrained params conv.weight doesn't match.(Pretrained: (19, 64, 3, 3), Actual: [2, 64, 3, 3])\n",
      "2021-05-06 21:35:53 [WARNING]\t[SKIP] Shape of pretrained params conv.bias doesn't match.(Pretrained: (19,), Actual: [2])\n",
      "2021-05-06 21:35:53 [INFO]\tThere are 108/112 variables loaded into UNet.\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.FP32, but right dtype is VarType.BOOL, the right dtype will convert to VarType.FP32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "2021-05-06 21:36:00 [INFO]\t[TRAIN] epoch=1, iter=10/16000, loss=0.3890, lr=0.001249, batch_cost=0.6965, reader_cost=0.44657, ips=5.7427 samples/sec | ETA 03:05:37\n",
      "2021-05-06 21:36:07 [INFO]\t[TRAIN] epoch=1, iter=20/16000, loss=0.2249, lr=0.001249, batch_cost=0.6966, reader_cost=0.44981, ips=5.7418 samples/sec | ETA 03:05:32\n",
      "2021-05-06 21:36:14 [INFO]\t[TRAIN] epoch=1, iter=30/16000, loss=0.1861, lr=0.001248, batch_cost=0.6654, reader_cost=0.41804, ips=6.0111 samples/sec | ETA 02:57:07\n",
      "2021-05-06 21:36:27 [INFO]\t[TRAIN] epoch=1, iter=50/16000, loss=0.1525, lr=0.001247, batch_cost=0.6963, reader_cost=0.44867, ips=5.7447 samples/sec | ETA 03:05:05\n",
      "2021-05-06 21:36:34 [INFO]\t[TRAIN] epoch=1, iter=60/16000, loss=0.1330, lr=0.001246, batch_cost=0.6738, reader_cost=0.42596, ips=5.9365 samples/sec | ETA 02:59:00\n",
      "2021-05-06 21:36:48 [INFO]\t[TRAIN] epoch=1, iter=80/16000, loss=0.0970, lr=0.001244, batch_cost=0.6818, reader_cost=0.43410, ips=5.8668 samples/sec | ETA 03:00:54\n",
      "2021-05-06 21:36:55 [INFO]\t[TRAIN] epoch=1, iter=90/16000, loss=0.0784, lr=0.001244, batch_cost=0.6780, reader_cost=0.43025, ips=5.8994 samples/sec | ETA 02:59:47\n",
      "2021-05-06 21:37:02 [INFO]\t[TRAIN] epoch=1, iter=100/16000, loss=0.0682, lr=0.001243, batch_cost=0.6869, reader_cost=0.43984, ips=5.8232 samples/sec | ETA 03:02:01\n",
      "2021-05-06 21:37:09 [INFO]\t[TRAIN] epoch=1, iter=110/16000, loss=0.0643, lr=0.001242, batch_cost=0.6847, reader_cost=0.43800, ips=5.8421 samples/sec | ETA 03:01:19\n",
      "2021-05-06 21:37:15 [INFO]\t[TRAIN] epoch=1, iter=120/16000, loss=0.0534, lr=0.001242, batch_cost=0.6835, reader_cost=0.43604, ips=5.8519 samples/sec | ETA 03:00:54\n",
      "2021-05-06 21:37:22 [INFO]\t[TRAIN] epoch=1, iter=130/16000, loss=0.0476, lr=0.001241, batch_cost=0.6859, reader_cost=0.43780, ips=5.8321 samples/sec | ETA 03:01:24\n",
      "2021-05-06 21:37:29 [INFO]\t[TRAIN] epoch=1, iter=140/16000, loss=0.0537, lr=0.001240, batch_cost=0.7149, reader_cost=0.46687, ips=5.5949 samples/sec | ETA 03:08:58\n",
      "2021-05-06 21:37:36 [INFO]\t[TRAIN] epoch=1, iter=150/16000, loss=0.0478, lr=0.001240, batch_cost=0.6859, reader_cost=0.43730, ips=5.8316 samples/sec | ETA 03:01:11\n",
      "2021-05-06 21:37:43 [INFO]\t[TRAIN] epoch=1, iter=160/16000, loss=0.0484, lr=0.001239, batch_cost=0.6792, reader_cost=0.43173, ips=5.8897 samples/sec | ETA 02:59:17\n",
      "2021-05-06 21:37:50 [INFO]\t[TRAIN] epoch=2, iter=170/16000, loss=0.0476, lr=0.001238, batch_cost=0.6918, reader_cost=0.44329, ips=5.7817 samples/sec | ETA 03:02:31\n",
      "2021-05-06 21:37:57 [INFO]\t[TRAIN] epoch=2, iter=180/16000, loss=0.0415, lr=0.001237, batch_cost=0.6827, reader_cost=0.43392, ips=5.8593 samples/sec | ETA 03:00:00\n",
      "2021-05-06 21:38:04 [INFO]\t[TRAIN] epoch=2, iter=190/16000, loss=0.0400, lr=0.001237, batch_cost=0.6822, reader_cost=0.43418, ips=5.8633 samples/sec | ETA 02:59:45\n",
      "2021-05-06 21:38:11 [INFO]\t[TRAIN] epoch=2, iter=200/16000, loss=0.0406, lr=0.001236, batch_cost=0.6852, reader_cost=0.43737, ips=5.8375 samples/sec | ETA 03:00:26\n",
      "2021-05-06 21:38:17 [INFO]\t[TRAIN] epoch=2, iter=210/16000, loss=0.0395, lr=0.001235, batch_cost=0.6912, reader_cost=0.44401, ips=5.7870 samples/sec | ETA 03:01:54\n",
      "2021-05-06 21:38:24 [INFO]\t[TRAIN] epoch=2, iter=220/16000, loss=0.0312, lr=0.001235, batch_cost=0.6661, reader_cost=0.41757, ips=6.0053 samples/sec | ETA 02:55:10\n",
      "2021-05-06 21:38:31 [INFO]\t[TRAIN] epoch=2, iter=230/16000, loss=0.0267, lr=0.001234, batch_cost=0.7174, reader_cost=0.46976, ips=5.5756 samples/sec | ETA 03:08:33\n",
      "2021-05-06 21:38:38 [INFO]\t[TRAIN] epoch=2, iter=240/16000, loss=0.0290, lr=0.001233, batch_cost=0.7081, reader_cost=0.46002, ips=5.6490 samples/sec | ETA 03:05:59\n",
      "2021-05-06 21:38:45 [INFO]\t[TRAIN] epoch=2, iter=250/16000, loss=0.0347, lr=0.001232, batch_cost=0.6690, reader_cost=0.42066, ips=5.9795 samples/sec | ETA 02:55:36\n",
      "2021-05-06 21:38:52 [INFO]\t[TRAIN] epoch=2, iter=260/16000, loss=0.0370, lr=0.001232, batch_cost=0.6646, reader_cost=0.41679, ips=6.0183 samples/sec | ETA 02:54:21\n",
      "2021-05-06 21:38:58 [INFO]\t[TRAIN] epoch=2, iter=270/16000, loss=0.0303, lr=0.001231, batch_cost=0.6639, reader_cost=0.41626, ips=6.0249 samples/sec | ETA 02:54:03\n",
      "2021-05-06 21:39:05 [INFO]\t[TRAIN] epoch=2, iter=280/16000, loss=0.0364, lr=0.001230, batch_cost=0.6898, reader_cost=0.44206, ips=5.7991 samples/sec | ETA 03:00:42\n",
      "2021-05-06 21:39:12 [INFO]\t[TRAIN] epoch=2, iter=290/16000, loss=0.0387, lr=0.001230, batch_cost=0.6668, reader_cost=0.41906, ips=5.9988 samples/sec | ETA 02:54:35\n",
      "2021-05-06 21:39:19 [INFO]\t[TRAIN] epoch=2, iter=300/16000, loss=0.0266, lr=0.001229, batch_cost=0.6780, reader_cost=0.42789, ips=5.8995 samples/sec | ETA 02:57:24\n",
      "2021-05-06 21:39:26 [INFO]\t[TRAIN] epoch=2, iter=310/16000, loss=0.0261, lr=0.001228, batch_cost=0.7074, reader_cost=0.44883, ips=5.6545 samples/sec | ETA 03:04:59\n",
      "2021-05-06 21:39:33 [INFO]\t[TRAIN] epoch=2, iter=320/16000, loss=0.0290, lr=0.001228, batch_cost=0.6821, reader_cost=0.43370, ips=5.8641 samples/sec | ETA 02:58:15\n",
      "2021-05-06 21:39:39 [INFO]\t[TRAIN] epoch=3, iter=330/16000, loss=0.0258, lr=0.001227, batch_cost=0.6804, reader_cost=0.43175, ips=5.8793 samples/sec | ETA 02:57:41\n",
      "2021-05-06 21:39:46 [INFO]\t[TRAIN] epoch=3, iter=340/16000, loss=0.0326, lr=0.001226, batch_cost=0.6942, reader_cost=0.44606, ips=5.7620 samples/sec | ETA 03:01:11\n",
      "2021-05-06 21:39:53 [INFO]\t[TRAIN] epoch=3, iter=350/16000, loss=0.0218, lr=0.001225, batch_cost=0.6671, reader_cost=0.41877, ips=5.9966 samples/sec | ETA 02:53:59\n",
      "2021-05-06 21:40:07 [INFO]\t[TRAIN] epoch=3, iter=370/16000, loss=0.0245, lr=0.001224, batch_cost=0.6842, reader_cost=0.43576, ips=5.8461 samples/sec | ETA 02:58:14\n",
      "2021-05-06 21:40:13 [INFO]\t[TRAIN] epoch=3, iter=380/16000, loss=0.0214, lr=0.001223, batch_cost=0.6622, reader_cost=0.41428, ips=6.0402 samples/sec | ETA 02:52:24\n",
      "2021-05-06 21:40:21 [INFO]\t[TRAIN] epoch=3, iter=390/16000, loss=0.0334, lr=0.001223, batch_cost=0.7074, reader_cost=0.46011, ips=5.6546 samples/sec | ETA 03:04:02\n",
      "2021-05-06 21:40:27 [INFO]\t[TRAIN] epoch=3, iter=400/16000, loss=0.0303, lr=0.001222, batch_cost=0.6782, reader_cost=0.43061, ips=5.8983 samples/sec | ETA 02:56:19\n",
      "2021-05-06 21:40:34 [INFO]\t[TRAIN] epoch=3, iter=410/16000, loss=0.0275, lr=0.001221, batch_cost=0.6617, reader_cost=0.41367, ips=6.0447 samples/sec | ETA 02:51:56\n",
      "2021-05-06 21:40:41 [INFO]\t[TRAIN] epoch=3, iter=420/16000, loss=0.0311, lr=0.001221, batch_cost=0.6858, reader_cost=0.43831, ips=5.8324 samples/sec | ETA 02:58:05\n",
      "2021-05-06 21:40:48 [INFO]\t[TRAIN] epoch=3, iter=430/16000, loss=0.0183, lr=0.001220, batch_cost=0.6873, reader_cost=0.43834, ips=5.8197 samples/sec | ETA 02:58:21\n",
      "2021-05-06 21:40:54 [INFO]\t[TRAIN] epoch=3, iter=440/16000, loss=0.0307, lr=0.001219, batch_cost=0.6683, reader_cost=0.42037, ips=5.9857 samples/sec | ETA 02:53:18\n",
      "2021-05-06 21:41:01 [INFO]\t[TRAIN] epoch=3, iter=450/16000, loss=0.0247, lr=0.001218, batch_cost=0.6958, reader_cost=0.44824, ips=5.7488 samples/sec | ETA 03:00:19\n",
      "2021-05-06 21:41:08 [INFO]\t[TRAIN] epoch=3, iter=460/16000, loss=0.0239, lr=0.001218, batch_cost=0.6945, reader_cost=0.43785, ips=5.7595 samples/sec | ETA 02:59:52\n",
      "2021-05-06 21:41:15 [INFO]\t[TRAIN] epoch=3, iter=470/16000, loss=0.0273, lr=0.001217, batch_cost=0.6713, reader_cost=0.42140, ips=5.9582 samples/sec | ETA 02:53:46\n",
      "2021-05-06 21:41:22 [INFO]\t[TRAIN] epoch=3, iter=480/16000, loss=0.0299, lr=0.001216, batch_cost=0.6668, reader_cost=0.41791, ips=5.9990 samples/sec | ETA 02:52:28\n",
      "2021-05-06 21:41:22 [INFO]\tStart evaluating (total_samples=160, total_iters=160)...\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.INT32, but right dtype is VarType.BOOL, the right dtype will convert to VarType.INT32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.INT64, but right dtype is VarType.BOOL, the right dtype will convert to VarType.INT64\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "155/160 [============================>.] - ETA: 0s - batch_cost: 0.1146 - reader cost: 0.1012021-05-06 21:41:49 [INFO]\t[TRAIN] epoch=4, iter=490/16000, loss=0.0300, lr=0.001216, batch_cost=0.6835, reader_cost=0.43123, ips=5.8522 samples/sec | ETA 02:56:41\n",
      "2021-05-06 21:41:56 [INFO]\t[TRAIN] epoch=4, iter=500/16000, loss=0.0223, lr=0.001215, batch_cost=0.6864, reader_cost=0.43835, ips=5.8279 samples/sec | ETA 02:57:18\n",
      "2021-05-06 21:42:02 [INFO]\t[TRAIN] epoch=4, iter=510/16000, loss=0.0220, lr=0.001214, batch_cost=0.6925, reader_cost=0.44486, ips=5.7762 samples/sec | ETA 02:58:46\n",
      "2021-05-06 21:42:09 [INFO]\t[TRAIN] epoch=4, iter=520/16000, loss=0.0297, lr=0.001213, batch_cost=0.6947, reader_cost=0.44515, ips=5.7576 samples/sec | ETA 02:59:14\n",
      "2021-05-06 21:42:16 [INFO]\t[TRAIN] epoch=4, iter=530/16000, loss=0.0267, lr=0.001213, batch_cost=0.6906, reader_cost=0.44250, ips=5.7917 samples/sec | ETA 02:58:04\n",
      "2021-05-06 21:42:23 [INFO]\t[TRAIN] epoch=4, iter=540/16000, loss=0.0309, lr=0.001212, batch_cost=0.6956, reader_cost=0.44746, ips=5.7504 samples/sec | ETA 02:59:14\n",
      "2021-05-06 21:42:30 [INFO]\t[TRAIN] epoch=4, iter=550/16000, loss=0.0225, lr=0.001211, batch_cost=0.6801, reader_cost=0.43222, ips=5.8812 samples/sec | ETA 02:55:08\n",
      "2021-05-06 21:42:37 [INFO]\t[TRAIN] epoch=4, iter=560/16000, loss=0.0250, lr=0.001211, batch_cost=0.6784, reader_cost=0.42834, ips=5.8959 samples/sec | ETA 02:54:34\n",
      "2021-05-06 21:42:43 [INFO]\t[TRAIN] epoch=4, iter=570/16000, loss=0.0222, lr=0.001210, batch_cost=0.6540, reader_cost=0.40534, ips=6.1164 samples/sec | ETA 02:48:10\n",
      "2021-05-06 21:42:50 [INFO]\t[TRAIN] epoch=4, iter=580/16000, loss=0.0268, lr=0.001209, batch_cost=0.6608, reader_cost=0.41121, ips=6.0531 samples/sec | ETA 02:49:49\n",
      "2021-05-06 21:42:57 [INFO]\t[TRAIN] epoch=4, iter=590/16000, loss=0.0193, lr=0.001209, batch_cost=0.6582, reader_cost=0.41032, ips=6.0774 samples/sec | ETA 02:49:02\n",
      "2021-05-06 21:43:03 [INFO]\t[TRAIN] epoch=4, iter=600/16000, loss=0.0177, lr=0.001208, batch_cost=0.6700, reader_cost=0.42208, ips=5.9704 samples/sec | ETA 02:51:57\n",
      "2021-05-06 21:43:10 [INFO]\t[TRAIN] epoch=4, iter=610/16000, loss=0.0207, lr=0.001207, batch_cost=0.6960, reader_cost=0.44843, ips=5.7473 samples/sec | ETA 02:58:31\n",
      "2021-05-06 21:43:17 [INFO]\t[TRAIN] epoch=4, iter=620/16000, loss=0.0218, lr=0.001206, batch_cost=0.6995, reader_cost=0.45183, ips=5.7181 samples/sec | ETA 02:59:18\n",
      "2021-05-06 21:43:24 [INFO]\t[TRAIN] epoch=4, iter=630/16000, loss=0.0278, lr=0.001206, batch_cost=0.6704, reader_cost=0.42332, ips=5.9665 samples/sec | ETA 02:51:44\n",
      "2021-05-06 21:43:31 [INFO]\t[TRAIN] epoch=4, iter=640/16000, loss=0.0244, lr=0.001205, batch_cost=0.6638, reader_cost=0.41578, ips=6.0256 samples/sec | ETA 02:49:56\n",
      "2021-05-06 21:43:37 [INFO]\t[TRAIN] epoch=5, iter=650/16000, loss=0.0224, lr=0.001204, batch_cost=0.6763, reader_cost=0.42820, ips=5.9148 samples/sec | ETA 02:53:00\n",
      "2021-05-06 21:43:44 [INFO]\t[TRAIN] epoch=5, iter=660/16000, loss=0.0163, lr=0.001204, batch_cost=0.7076, reader_cost=0.46010, ips=5.6528 samples/sec | ETA 03:00:54\n",
      "2021-05-06 21:43:51 [INFO]\t[TRAIN] epoch=5, iter=670/16000, loss=0.0173, lr=0.001203, batch_cost=0.6601, reader_cost=0.41280, ips=6.0600 samples/sec | ETA 02:48:38\n",
      "2021-05-06 21:43:58 [INFO]\t[TRAIN] epoch=5, iter=680/16000, loss=0.0272, lr=0.001202, batch_cost=0.6633, reader_cost=0.41631, ips=6.0302 samples/sec | ETA 02:49:22\n",
      "2021-05-06 21:44:05 [INFO]\t[TRAIN] epoch=5, iter=690/16000, loss=0.0297, lr=0.001201, batch_cost=0.6927, reader_cost=0.43482, ips=5.7747 samples/sec | ETA 02:56:44\n",
      "2021-05-06 21:44:11 [INFO]\t[TRAIN] epoch=5, iter=700/16000, loss=0.0228, lr=0.001201, batch_cost=0.6682, reader_cost=0.42002, ips=5.9861 samples/sec | ETA 02:50:23\n",
      "2021-05-06 21:44:18 [INFO]\t[TRAIN] epoch=5, iter=710/16000, loss=0.0319, lr=0.001200, batch_cost=0.6779, reader_cost=0.43066, ips=5.9005 samples/sec | ETA 02:52:45\n",
      "2021-05-06 21:44:25 [INFO]\t[TRAIN] epoch=5, iter=720/16000, loss=0.0275, lr=0.001199, batch_cost=0.7136, reader_cost=0.46445, ips=5.6051 samples/sec | ETA 03:01:44\n",
      "2021-05-06 21:44:32 [INFO]\t[TRAIN] epoch=5, iter=730/16000, loss=0.0228, lr=0.001199, batch_cost=0.6671, reader_cost=0.41867, ips=5.9957 samples/sec | ETA 02:49:47\n",
      "2021-05-06 21:44:39 [INFO]\t[TRAIN] epoch=5, iter=740/16000, loss=0.0175, lr=0.001198, batch_cost=0.6824, reader_cost=0.43430, ips=5.8618 samples/sec | ETA 02:53:33\n",
      "2021-05-06 21:44:46 [INFO]\t[TRAIN] epoch=5, iter=750/16000, loss=0.0319, lr=0.001197, batch_cost=0.6921, reader_cost=0.43096, ips=5.7793 samples/sec | ETA 02:55:54\n",
      "2021-05-06 21:44:53 [INFO]\t[TRAIN] epoch=5, iter=760/16000, loss=0.0217, lr=0.001197, batch_cost=0.7010, reader_cost=0.45079, ips=5.7064 samples/sec | ETA 02:58:02\n",
      "2021-05-06 21:44:59 [INFO]\t[TRAIN] epoch=5, iter=770/16000, loss=0.0169, lr=0.001196, batch_cost=0.6721, reader_cost=0.42451, ips=5.9516 samples/sec | ETA 02:50:35\n",
      "2021-05-06 21:45:06 [INFO]\t[TRAIN] epoch=5, iter=780/16000, loss=0.0148, lr=0.001195, batch_cost=0.7011, reader_cost=0.45182, ips=5.7055 samples/sec | ETA 02:57:50\n",
      "2021-05-06 21:45:14 [INFO]\t[TRAIN] epoch=5, iter=790/16000, loss=0.0187, lr=0.001194, batch_cost=0.7196, reader_cost=0.47158, ips=5.5589 samples/sec | ETA 03:02:24\n",
      "2021-05-06 21:45:20 [INFO]\t[TRAIN] epoch=5, iter=800/16000, loss=0.0231, lr=0.001194, batch_cost=0.6681, reader_cost=0.42075, ips=5.9871 samples/sec | ETA 02:49:15\n",
      "2021-05-06 21:45:27 [INFO]\t[TRAIN] epoch=6, iter=810/16000, loss=0.0257, lr=0.001193, batch_cost=0.6877, reader_cost=0.43899, ips=5.8163 samples/sec | ETA 02:54:06\n",
      "2021-05-06 21:45:34 [INFO]\t[TRAIN] epoch=6, iter=820/16000, loss=0.0169, lr=0.001192, batch_cost=0.6692, reader_cost=0.41991, ips=5.9771 samples/sec | ETA 02:49:18\n",
      "2021-05-06 21:45:41 [INFO]\t[TRAIN] epoch=6, iter=830/16000, loss=0.0163, lr=0.001192, batch_cost=0.6883, reader_cost=0.44046, ips=5.8114 samples/sec | ETA 02:54:01\n",
      "2021-05-06 21:45:48 [INFO]\t[TRAIN] epoch=6, iter=840/16000, loss=0.0161, lr=0.001191, batch_cost=0.7071, reader_cost=0.45823, ips=5.6569 samples/sec | ETA 02:58:39\n",
      "2021-05-06 21:45:55 [INFO]\t[TRAIN] epoch=6, iter=850/16000, loss=0.0219, lr=0.001190, batch_cost=0.6860, reader_cost=0.43557, ips=5.8312 samples/sec | ETA 02:53:12\n",
      "2021-05-06 21:46:01 [INFO]\t[TRAIN] epoch=6, iter=860/16000, loss=0.0179, lr=0.001189, batch_cost=0.6798, reader_cost=0.43146, ips=5.8845 samples/sec | ETA 02:51:31\n",
      "2021-05-06 21:46:08 [INFO]\t[TRAIN] epoch=6, iter=870/16000, loss=0.0198, lr=0.001189, batch_cost=0.6962, reader_cost=0.44838, ips=5.7452 samples/sec | ETA 02:55:33\n",
      "2021-05-06 21:46:15 [INFO]\t[TRAIN] epoch=6, iter=880/16000, loss=0.0251, lr=0.001188, batch_cost=0.6906, reader_cost=0.44308, ips=5.7922 samples/sec | ETA 02:54:01\n",
      "2021-05-06 21:46:29 [INFO]\t[TRAIN] epoch=6, iter=900/16000, loss=0.0210, lr=0.001187, batch_cost=0.6898, reader_cost=0.44003, ips=5.7989 samples/sec | ETA 02:53:35\n",
      "2021-05-06 21:46:36 [INFO]\t[TRAIN] epoch=6, iter=910/16000, loss=0.0156, lr=0.001186, batch_cost=0.7024, reader_cost=0.45371, ips=5.6951 samples/sec | ETA 02:56:38\n",
      "2021-05-06 21:46:43 [INFO]\t[TRAIN] epoch=6, iter=920/16000, loss=0.0244, lr=0.001185, batch_cost=0.6921, reader_cost=0.43002, ips=5.7799 samples/sec | ETA 02:53:56\n",
      "2021-05-06 21:46:49 [INFO]\t[TRAIN] epoch=6, iter=930/16000, loss=0.0171, lr=0.001184, batch_cost=0.6740, reader_cost=0.42648, ips=5.9343 samples/sec | ETA 02:49:17\n",
      "2021-05-06 21:46:56 [INFO]\t[TRAIN] epoch=6, iter=940/16000, loss=0.0302, lr=0.001184, batch_cost=0.6786, reader_cost=0.43080, ips=5.8942 samples/sec | ETA 02:50:20\n",
      "2021-05-06 21:47:03 [INFO]\t[TRAIN] epoch=6, iter=950/16000, loss=0.0209, lr=0.001183, batch_cost=0.6816, reader_cost=0.43309, ips=5.8689 samples/sec | ETA 02:50:57\n",
      "2021-05-06 21:47:10 [INFO]\t[TRAIN] epoch=6, iter=960/16000, loss=0.0184, lr=0.001182, batch_cost=0.7144, reader_cost=0.46580, ips=5.5992 samples/sec | ETA 02:59:04\n",
      "2021-05-06 21:47:10 [INFO]\tStart evaluating (total_samples=160, total_iters=160)...\n",
      "160/160 [==============================] - 18s 115ms/step - batch_cost: 0.1142 - reader cost: 0.101\n",
      "2021-05-06 21:47:28 [INFO]\t[EVAL] #Images=160 mIoU=0.9323 Acc=0.9976 Kappa=0.9276 \n",
      "2021-05-06 21:47:28 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9976 0.867 ]\n",
      "2021-05-06 21:47:28 [INFO]\t[EVAL] Class Acc: \n",
      "[0.999  0.9188]\n",
      "2021-05-06 21:47:30 [INFO]\t[EVAL] The model with the best validation mIoU (0.9323) was saved at iter 960.\n",
      "2021-05-06 21:47:38 [INFO]\t[TRAIN] epoch=7, iter=970/16000, loss=0.0219, lr=0.001182, batch_cost=0.7230, reader_cost=0.47237, ips=5.5326 samples/sec | ETA 03:01:06\n",
      "2021-05-06 21:47:45 [INFO]\t[TRAIN] epoch=7, iter=980/16000, loss=0.0215, lr=0.001181, batch_cost=0.6984, reader_cost=0.45042, ips=5.7274 samples/sec | ETA 02:54:49\n",
      "2021-05-06 21:47:51 [INFO]\t[TRAIN] epoch=7, iter=990/16000, loss=0.0161, lr=0.001180, batch_cost=0.6720, reader_cost=0.42349, ips=5.9525 samples/sec | ETA 02:48:06\n",
      "2021-05-06 21:47:58 [INFO]\t[TRAIN] epoch=7, iter=1000/16000, loss=0.0238, lr=0.001180, batch_cost=0.6752, reader_cost=0.42684, ips=5.9239 samples/sec | ETA 02:48:48\n",
      "2021-05-06 21:48:05 [INFO]\t[TRAIN] epoch=7, iter=1010/16000, loss=0.0208, lr=0.001179, batch_cost=0.6784, reader_cost=0.43062, ips=5.8961 samples/sec | ETA 02:49:29\n",
      "2021-05-06 21:48:12 [INFO]\t[TRAIN] epoch=7, iter=1020/16000, loss=0.0235, lr=0.001178, batch_cost=0.6825, reader_cost=0.43454, ips=5.8609 samples/sec | ETA 02:50:23\n",
      "2021-05-06 21:48:19 [INFO]\t[TRAIN] epoch=7, iter=1030/16000, loss=0.0198, lr=0.001177, batch_cost=0.6815, reader_cost=0.43293, ips=5.8694 samples/sec | ETA 02:50:02\n",
      "2021-05-06 21:48:25 [INFO]\t[TRAIN] epoch=7, iter=1040/16000, loss=0.0223, lr=0.001177, batch_cost=0.6239, reader_cost=0.37475, ips=6.4118 samples/sec | ETA 02:35:32\n",
      "2021-05-06 21:48:32 [INFO]\t[TRAIN] epoch=7, iter=1050/16000, loss=0.0167, lr=0.001176, batch_cost=0.7209, reader_cost=0.47190, ips=5.5489 samples/sec | ETA 02:59:36\n",
      "2021-05-06 21:48:39 [INFO]\t[TRAIN] epoch=7, iter=1060/16000, loss=0.0150, lr=0.001175, batch_cost=0.6956, reader_cost=0.44723, ips=5.7503 samples/sec | ETA 02:53:12\n",
      "2021-05-06 21:48:46 [INFO]\t[TRAIN] epoch=7, iter=1070/16000, loss=0.0135, lr=0.001175, batch_cost=0.6920, reader_cost=0.44397, ips=5.7806 samples/sec | ETA 02:52:11\n",
      "2021-05-06 21:48:53 [INFO]\t[TRAIN] epoch=7, iter=1080/16000, loss=0.0212, lr=0.001174, batch_cost=0.6945, reader_cost=0.44613, ips=5.7597 samples/sec | ETA 02:52:41\n",
      "2021-05-06 21:48:59 [INFO]\t[TRAIN] epoch=7, iter=1090/16000, loss=0.0203, lr=0.001173, batch_cost=0.6591, reader_cost=0.40730, ips=6.0686 samples/sec | ETA 02:43:47\n",
      "2021-05-06 21:49:06 [INFO]\t[TRAIN] epoch=7, iter=1100/16000, loss=0.0174, lr=0.001172, batch_cost=0.6714, reader_cost=0.42094, ips=5.9578 samples/sec | ETA 02:46:43\n",
      "2021-05-06 21:49:13 [INFO]\t[TRAIN] epoch=7, iter=1110/16000, loss=0.0220, lr=0.001172, batch_cost=0.6726, reader_cost=0.42358, ips=5.9473 samples/sec | ETA 02:46:54\n",
      "2021-05-06 21:49:20 [INFO]\t[TRAIN] epoch=7, iter=1120/16000, loss=0.0183, lr=0.001171, batch_cost=0.6863, reader_cost=0.43734, ips=5.8280 samples/sec | ETA 02:50:12\n",
      "2021-05-06 21:49:26 [INFO]\t[TRAIN] epoch=8, iter=1130/16000, loss=0.0200, lr=0.001170, batch_cost=0.6651, reader_cost=0.41715, ips=6.0140 samples/sec | ETA 02:44:50\n",
      "2021-05-06 21:49:33 [INFO]\t[TRAIN] epoch=8, iter=1140/16000, loss=0.0186, lr=0.001170, batch_cost=0.6679, reader_cost=0.41994, ips=5.9887 samples/sec | ETA 02:45:25\n",
      "2021-05-06 21:49:40 [INFO]\t[TRAIN] epoch=8, iter=1150/16000, loss=0.0172, lr=0.001169, batch_cost=0.7118, reader_cost=0.46442, ips=5.6195 samples/sec | ETA 02:56:10\n",
      "2021-05-06 21:49:47 [INFO]\t[TRAIN] epoch=8, iter=1160/16000, loss=0.0151, lr=0.001168, batch_cost=0.6864, reader_cost=0.43889, ips=5.8279 samples/sec | ETA 02:49:45\n",
      "2021-05-06 21:49:54 [INFO]\t[TRAIN] epoch=8, iter=1170/16000, loss=0.0226, lr=0.001167, batch_cost=0.7073, reader_cost=0.45958, ips=5.6551 samples/sec | ETA 02:54:49\n",
      "2021-05-06 21:50:01 [INFO]\t[TRAIN] epoch=8, iter=1180/16000, loss=0.0243, lr=0.001167, batch_cost=0.6672, reader_cost=0.41645, ips=5.9949 samples/sec | ETA 02:44:48\n",
      "2021-05-06 21:50:08 [INFO]\t[TRAIN] epoch=8, iter=1190/16000, loss=0.0171, lr=0.001166, batch_cost=0.6890, reader_cost=0.43968, ips=5.8054 samples/sec | ETA 02:50:04\n",
      "2021-05-06 21:50:15 [INFO]\t[TRAIN] epoch=8, iter=1200/16000, loss=0.0223, lr=0.001165, batch_cost=0.6927, reader_cost=0.44439, ips=5.7741 samples/sec | ETA 02:50:52\n",
      "2021-05-06 21:50:22 [INFO]\t[TRAIN] epoch=8, iter=1210/16000, loss=0.0172, lr=0.001165, batch_cost=0.6957, reader_cost=0.43837, ips=5.7500 samples/sec | ETA 02:51:28\n",
      "2021-05-06 21:50:28 [INFO]\t[TRAIN] epoch=8, iter=1220/16000, loss=0.0210, lr=0.001164, batch_cost=0.6622, reader_cost=0.41348, ips=6.0403 samples/sec | ETA 02:43:07\n",
      "2021-05-06 21:50:35 [INFO]\t[TRAIN] epoch=8, iter=1230/16000, loss=0.0139, lr=0.001163, batch_cost=0.6960, reader_cost=0.44834, ips=5.7473 samples/sec | ETA 02:51:19\n",
      "2021-05-06 21:50:42 [INFO]\t[TRAIN] epoch=8, iter=1240/16000, loss=0.0159, lr=0.001163, batch_cost=0.6893, reader_cost=0.44034, ips=5.8029 samples/sec | ETA 02:49:34\n",
      "2021-05-06 21:50:49 [INFO]\t[TRAIN] epoch=8, iter=1250/16000, loss=0.0175, lr=0.001162, batch_cost=0.6789, reader_cost=0.43074, ips=5.8916 samples/sec | ETA 02:46:54\n",
      "2021-05-06 21:50:56 [INFO]\t[TRAIN] epoch=8, iter=1260/16000, loss=0.0158, lr=0.001161, batch_cost=0.7333, reader_cost=0.48512, ips=5.4547 samples/sec | ETA 03:00:09\n",
      "2021-05-06 21:51:03 [INFO]\t[TRAIN] epoch=8, iter=1270/16000, loss=0.0203, lr=0.001160, batch_cost=0.6475, reader_cost=0.39900, ips=6.1778 samples/sec | ETA 02:38:57\n",
      "2021-05-06 21:51:09 [INFO]\t[TRAIN] epoch=8, iter=1280/16000, loss=0.0158, lr=0.001160, batch_cost=0.6765, reader_cost=0.42764, ips=5.9130 samples/sec | ETA 02:45:57\n",
      "2021-05-06 21:51:16 [INFO]\t[TRAIN] epoch=9, iter=1290/16000, loss=0.0169, lr=0.001159, batch_cost=0.6809, reader_cost=0.43306, ips=5.8744 samples/sec | ETA 02:46:56\n",
      "2021-05-06 21:51:23 [INFO]\t[TRAIN] epoch=9, iter=1300/16000, loss=0.0150, lr=0.001158, batch_cost=0.6878, reader_cost=0.43986, ips=5.8155 samples/sec | ETA 02:48:30\n",
      "2021-05-06 21:51:30 [INFO]\t[TRAIN] epoch=9, iter=1310/16000, loss=0.0146, lr=0.001158, batch_cost=0.6666, reader_cost=0.41820, ips=6.0006 samples/sec | ETA 02:43:12\n",
      "2021-05-06 21:51:36 [INFO]\t[TRAIN] epoch=9, iter=1320/16000, loss=0.0167, lr=0.001157, batch_cost=0.6718, reader_cost=0.42374, ips=5.9540 samples/sec | ETA 02:44:22\n",
      "2021-05-06 21:51:43 [INFO]\t[TRAIN] epoch=9, iter=1330/16000, loss=0.0156, lr=0.001156, batch_cost=0.6909, reader_cost=0.44225, ips=5.7892 samples/sec | ETA 02:48:56\n",
      "2021-05-06 21:51:50 [INFO]\t[TRAIN] epoch=9, iter=1340/16000, loss=0.0183, lr=0.001155, batch_cost=0.6617, reader_cost=0.41277, ips=6.0446 samples/sec | ETA 02:41:41\n",
      "2021-05-06 21:51:57 [INFO]\t[TRAIN] epoch=9, iter=1350/16000, loss=0.0158, lr=0.001155, batch_cost=0.6947, reader_cost=0.44683, ips=5.7576 samples/sec | ETA 02:49:37\n",
      "2021-05-06 21:52:04 [INFO]\t[TRAIN] epoch=9, iter=1360/16000, loss=0.0257, lr=0.001154, batch_cost=0.6829, reader_cost=0.43494, ips=5.8571 samples/sec | ETA 02:46:38\n",
      "2021-05-06 21:52:11 [INFO]\t[TRAIN] epoch=9, iter=1370/16000, loss=0.0154, lr=0.001153, batch_cost=0.6908, reader_cost=0.44288, ips=5.7900 samples/sec | ETA 02:48:27\n",
      "2021-05-06 21:52:18 [INFO]\t[TRAIN] epoch=9, iter=1380/16000, loss=0.0143, lr=0.001153, batch_cost=0.6813, reader_cost=0.43279, ips=5.8709 samples/sec | ETA 02:46:01\n",
      "2021-05-06 21:52:24 [INFO]\t[TRAIN] epoch=9, iter=1390/16000, loss=0.0170, lr=0.001152, batch_cost=0.6581, reader_cost=0.40893, ips=6.0784 samples/sec | ETA 02:40:14\n",
      "2021-05-06 21:52:31 [INFO]\t[TRAIN] epoch=9, iter=1400/16000, loss=0.0154, lr=0.001151, batch_cost=0.6971, reader_cost=0.44867, ips=5.7383 samples/sec | ETA 02:49:37\n",
      "2021-05-06 21:52:38 [INFO]\t[TRAIN] epoch=9, iter=1410/16000, loss=0.0205, lr=0.001150, batch_cost=0.7050, reader_cost=0.45727, ips=5.6741 samples/sec | ETA 02:51:25\n",
      "2021-05-06 21:52:45 [INFO]\t[TRAIN] epoch=9, iter=1420/16000, loss=0.0202, lr=0.001150, batch_cost=0.6674, reader_cost=0.41766, ips=5.9937 samples/sec | ETA 02:42:10\n",
      "2021-05-06 21:52:52 [INFO]\t[TRAIN] epoch=9, iter=1430/16000, loss=0.0189, lr=0.001149, batch_cost=0.7285, reader_cost=0.48097, ips=5.4907 samples/sec | ETA 02:56:54\n",
      "2021-05-06 21:52:59 [INFO]\t[TRAIN] epoch=9, iter=1440/16000, loss=0.0192, lr=0.001148, batch_cost=0.7203, reader_cost=0.46504, ips=5.5530 samples/sec | ETA 02:54:48\n",
      "2021-05-06 21:52:59 [INFO]\tStart evaluating (total_samples=160, total_iters=160)...\n",
      "160/160 [==============================] - 18s 114ms/step - batch_cost: 0.1137 - reader cost: 0.09\n",
      "2021-05-06 21:53:18 [INFO]\t[EVAL] #Images=160 mIoU=0.9357 Acc=0.9978 Kappa=0.9314 \n",
      "2021-05-06 21:53:18 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9977 0.8736]\n",
      "2021-05-06 21:53:18 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9989 0.9278]\n",
      "2021-05-06 21:53:19 [INFO]\t[EVAL] The model with the best validation mIoU (0.9357) was saved at iter 1440.\n",
      "2021-05-06 21:53:26 [INFO]\t[TRAIN] epoch=10, iter=1450/16000, loss=0.0177, lr=0.001148, batch_cost=0.7045, reader_cost=0.45523, ips=5.6776 samples/sec | ETA 02:50:50\n",
      "2021-05-06 21:53:33 [INFO]\t[TRAIN] epoch=10, iter=1460/16000, loss=0.0124, lr=0.001147, batch_cost=0.6708, reader_cost=0.42247, ips=5.9633 samples/sec | ETA 02:42:32\n",
      "2021-05-06 21:53:40 [INFO]\t[TRAIN] epoch=10, iter=1470/16000, loss=0.0160, lr=0.001146, batch_cost=0.6601, reader_cost=0.41046, ips=6.0599 samples/sec | ETA 02:39:50\n",
      "2021-05-06 21:53:47 [INFO]\t[TRAIN] epoch=10, iter=1480/16000, loss=0.0173, lr=0.001146, batch_cost=0.6766, reader_cost=0.42782, ips=5.9117 samples/sec | ETA 02:43:44\n",
      "2021-05-06 21:53:54 [INFO]\t[TRAIN] epoch=10, iter=1490/16000, loss=0.0155, lr=0.001145, batch_cost=0.7090, reader_cost=0.46036, ips=5.6415 samples/sec | ETA 02:51:28\n",
      "2021-05-06 21:54:01 [INFO]\t[TRAIN] epoch=10, iter=1500/16000, loss=0.0125, lr=0.001144, batch_cost=0.6893, reader_cost=0.44159, ips=5.8029 samples/sec | ETA 02:46:35\n",
      "2021-05-06 21:54:07 [INFO]\t[TRAIN] epoch=10, iter=1510/16000, loss=0.0154, lr=0.001143, batch_cost=0.6773, reader_cost=0.42911, ips=5.9057 samples/sec | ETA 02:43:34\n",
      "2021-05-06 21:54:14 [INFO]\t[TRAIN] epoch=10, iter=1520/16000, loss=0.0215, lr=0.001143, batch_cost=0.6418, reader_cost=0.39363, ips=6.2322 samples/sec | ETA 02:34:53\n",
      "2021-05-06 21:54:21 [INFO]\t[TRAIN] epoch=10, iter=1530/16000, loss=0.0139, lr=0.001142, batch_cost=0.6968, reader_cost=0.44874, ips=5.7408 samples/sec | ETA 02:48:02\n",
      "2021-05-06 21:54:28 [INFO]\t[TRAIN] epoch=10, iter=1540/16000, loss=0.0137, lr=0.001141, batch_cost=0.6874, reader_cost=0.43937, ips=5.8189 samples/sec | ETA 02:45:40\n",
      "2021-05-06 21:54:34 [INFO]\t[TRAIN] epoch=10, iter=1550/16000, loss=0.0135, lr=0.001141, batch_cost=0.6678, reader_cost=0.41927, ips=5.9896 samples/sec | ETA 02:40:50\n",
      "2021-05-06 21:54:41 [INFO]\t[TRAIN] epoch=10, iter=1560/16000, loss=0.0145, lr=0.001140, batch_cost=0.6915, reader_cost=0.44400, ips=5.7846 samples/sec | ETA 02:46:25\n",
      "2021-05-06 21:54:48 [INFO]\t[TRAIN] epoch=10, iter=1570/16000, loss=0.0187, lr=0.001139, batch_cost=0.7102, reader_cost=0.46271, ips=5.6323 samples/sec | ETA 02:50:47\n",
      "2021-05-06 21:54:55 [INFO]\t[TRAIN] epoch=10, iter=1580/16000, loss=0.0250, lr=0.001138, batch_cost=0.6545, reader_cost=0.40673, ips=6.1117 samples/sec | ETA 02:37:17\n",
      "2021-05-06 21:55:01 [INFO]\t[TRAIN] epoch=10, iter=1590/16000, loss=0.0217, lr=0.001138, batch_cost=0.6617, reader_cost=0.41352, ips=6.0446 samples/sec | ETA 02:38:55\n",
      "2021-05-06 21:55:08 [INFO]\t[TRAIN] epoch=10, iter=1600/16000, loss=0.0122, lr=0.001137, batch_cost=0.7065, reader_cost=0.45891, ips=5.6617 samples/sec | ETA 02:49:33\n",
      "2021-05-06 21:55:15 [INFO]\t[TRAIN] epoch=11, iter=1610/16000, loss=0.0176, lr=0.001136, batch_cost=0.6301, reader_cost=0.38091, ips=6.3477 samples/sec | ETA 02:31:07\n",
      "2021-05-06 21:55:21 [INFO]\t[TRAIN] epoch=11, iter=1620/16000, loss=0.0234, lr=0.001136, batch_cost=0.6467, reader_cost=0.39967, ips=6.1853 samples/sec | ETA 02:34:59\n",
      "2021-05-06 21:55:28 [INFO]\t[TRAIN] epoch=11, iter=1630/16000, loss=0.0118, lr=0.001135, batch_cost=0.6577, reader_cost=0.40928, ips=6.0818 samples/sec | ETA 02:37:31\n",
      "2021-05-06 21:55:35 [INFO]\t[TRAIN] epoch=11, iter=1640/16000, loss=0.0172, lr=0.001134, batch_cost=0.6706, reader_cost=0.42169, ips=5.9646 samples/sec | ETA 02:40:30\n",
      "2021-05-06 21:55:42 [INFO]\t[TRAIN] epoch=11, iter=1650/16000, loss=0.0147, lr=0.001133, batch_cost=0.7090, reader_cost=0.46023, ips=5.6418 samples/sec | ETA 02:49:34\n",
      "2021-05-06 21:55:49 [INFO]\t[TRAIN] epoch=11, iter=1660/16000, loss=0.0212, lr=0.001133, batch_cost=0.7247, reader_cost=0.47670, ips=5.5199 samples/sec | ETA 02:53:11\n",
      "2021-05-06 21:55:56 [INFO]\t[TRAIN] epoch=11, iter=1670/16000, loss=0.0186, lr=0.001132, batch_cost=0.6871, reader_cost=0.43909, ips=5.8215 samples/sec | ETA 02:44:06\n",
      "2021-05-06 21:56:02 [INFO]\t[TRAIN] epoch=11, iter=1680/16000, loss=0.0134, lr=0.001131, batch_cost=0.6640, reader_cost=0.41591, ips=6.0239 samples/sec | ETA 02:38:28\n",
      "2021-05-06 21:56:09 [INFO]\t[TRAIN] epoch=11, iter=1690/16000, loss=0.0148, lr=0.001131, batch_cost=0.6723, reader_cost=0.42326, ips=5.9499 samples/sec | ETA 02:40:20\n",
      "2021-05-06 21:56:16 [INFO]\t[TRAIN] epoch=11, iter=1700/16000, loss=0.0158, lr=0.001130, batch_cost=0.6997, reader_cost=0.45131, ips=5.7164 samples/sec | ETA 02:46:46\n",
      "2021-05-06 21:56:23 [INFO]\t[TRAIN] epoch=11, iter=1710/16000, loss=0.0182, lr=0.001129, batch_cost=0.6548, reader_cost=0.40689, ips=6.1090 samples/sec | ETA 02:35:56\n",
      "2021-05-06 21:56:30 [INFO]\t[TRAIN] epoch=11, iter=1720/16000, loss=0.0174, lr=0.001128, batch_cost=0.6933, reader_cost=0.44561, ips=5.7691 samples/sec | ETA 02:45:00\n",
      "2021-05-06 21:56:36 [INFO]\t[TRAIN] epoch=11, iter=1730/16000, loss=0.0154, lr=0.001128, batch_cost=0.6921, reader_cost=0.44387, ips=5.7795 samples/sec | ETA 02:44:36\n",
      "2021-05-06 21:56:43 [INFO]\t[TRAIN] epoch=11, iter=1740/16000, loss=0.0160, lr=0.001127, batch_cost=0.6922, reader_cost=0.44437, ips=5.7790 samples/sec | ETA 02:44:30\n",
      "2021-05-06 21:56:50 [INFO]\t[TRAIN] epoch=11, iter=1750/16000, loss=0.0131, lr=0.001126, batch_cost=0.6978, reader_cost=0.45038, ips=5.7322 samples/sec | ETA 02:45:43\n",
      "2021-05-06 21:56:57 [INFO]\t[TRAIN] epoch=11, iter=1760/16000, loss=0.0152, lr=0.001126, batch_cost=0.6928, reader_cost=0.44544, ips=5.7735 samples/sec | ETA 02:44:25\n",
      "2021-05-06 21:57:04 [INFO]\t[TRAIN] epoch=12, iter=1770/16000, loss=0.0156, lr=0.001125, batch_cost=0.7083, reader_cost=0.46075, ips=5.6475 samples/sec | ETA 02:47:58\n",
      "2021-05-06 21:57:11 [INFO]\t[TRAIN] epoch=12, iter=1780/16000, loss=0.0184, lr=0.001124, batch_cost=0.6669, reader_cost=0.41886, ips=5.9976 samples/sec | ETA 02:38:03\n",
      "2021-05-06 21:57:18 [INFO]\t[TRAIN] epoch=12, iter=1790/16000, loss=0.0119, lr=0.001123, batch_cost=0.7055, reader_cost=0.45768, ips=5.6696 samples/sec | ETA 02:47:05\n",
      "2021-05-06 21:57:25 [INFO]\t[TRAIN] epoch=12, iter=1800/16000, loss=0.0190, lr=0.001123, batch_cost=0.6722, reader_cost=0.42276, ips=5.9505 samples/sec | ETA 02:39:05\n",
      "2021-05-06 21:57:32 [INFO]\t[TRAIN] epoch=12, iter=1810/16000, loss=0.0194, lr=0.001122, batch_cost=0.6889, reader_cost=0.43142, ips=5.8065 samples/sec | ETA 02:42:55\n",
      "2021-05-06 21:57:39 [INFO]\t[TRAIN] epoch=12, iter=1820/16000, loss=0.0143, lr=0.001121, batch_cost=0.6760, reader_cost=0.42780, ips=5.9172 samples/sec | ETA 02:39:45\n",
      "2021-05-06 21:57:45 [INFO]\t[TRAIN] epoch=12, iter=1830/16000, loss=0.0198, lr=0.001121, batch_cost=0.6880, reader_cost=0.43930, ips=5.8142 samples/sec | ETA 02:42:28\n",
      "2021-05-06 21:57:52 [INFO]\t[TRAIN] epoch=12, iter=1840/16000, loss=0.0172, lr=0.001120, batch_cost=0.6872, reader_cost=0.43966, ips=5.8210 samples/sec | ETA 02:42:10\n",
      "2021-05-06 21:57:59 [INFO]\t[TRAIN] epoch=12, iter=1850/16000, loss=0.0126, lr=0.001119, batch_cost=0.7084, reader_cost=0.46072, ips=5.6463 samples/sec | ETA 02:47:04\n",
      "2021-05-06 21:58:06 [INFO]\t[TRAIN] epoch=12, iter=1860/16000, loss=0.0233, lr=0.001118, batch_cost=0.6777, reader_cost=0.42953, ips=5.9020 samples/sec | ETA 02:39:43\n",
      "2021-05-06 21:58:13 [INFO]\t[TRAIN] epoch=12, iter=1870/16000, loss=0.0171, lr=0.001118, batch_cost=0.6673, reader_cost=0.41835, ips=5.9942 samples/sec | ETA 02:37:09\n",
      "2021-05-06 21:58:19 [INFO]\t[TRAIN] epoch=12, iter=1880/16000, loss=0.0187, lr=0.001117, batch_cost=0.6661, reader_cost=0.41824, ips=6.0054 samples/sec | ETA 02:36:44\n",
      "2021-05-06 21:58:26 [INFO]\t[TRAIN] epoch=12, iter=1890/16000, loss=0.0144, lr=0.001116, batch_cost=0.6774, reader_cost=0.42961, ips=5.9052 samples/sec | ETA 02:39:17\n",
      "2021-05-06 21:58:33 [INFO]\t[TRAIN] epoch=12, iter=1900/16000, loss=0.0136, lr=0.001116, batch_cost=0.6888, reader_cost=0.44061, ips=5.8071 samples/sec | ETA 02:41:52\n",
      "2021-05-06 21:58:40 [INFO]\t[TRAIN] epoch=12, iter=1910/16000, loss=0.0136, lr=0.001115, batch_cost=0.7041, reader_cost=0.45605, ips=5.6814 samples/sec | ETA 02:45:20\n",
      "2021-05-06 21:58:47 [INFO]\t[TRAIN] epoch=12, iter=1920/16000, loss=0.0155, lr=0.001114, batch_cost=0.6559, reader_cost=0.40778, ips=6.0983 samples/sec | ETA 02:33:55\n",
      "2021-05-06 21:58:47 [INFO]\tStart evaluating (total_samples=160, total_iters=160)...\n",
      "160/160 [==============================] - 18s 114ms/step - batch_cost: 0.1133 - reader cost: 0.10\n",
      "2021-05-06 21:59:05 [INFO]\t[EVAL] #Images=160 mIoU=0.9399 Acc=0.9979 Kappa=0.9362 \n",
      "2021-05-06 21:59:05 [INFO]\t[EVAL] Class IoU: \n",
      "[0.9979 0.882 ]\n",
      "2021-05-06 21:59:05 [INFO]\t[EVAL] Class Acc: \n",
      "[0.9991 0.9298]\n",
      "2021-05-06 21:59:08 [INFO]\t[EVAL] The model with the best validation mIoU (0.9399) was saved at iter 1920.\n",
      "2021-05-06 21:59:15 [INFO]\t[TRAIN] epoch=13, iter=1930/16000, loss=0.0186, lr=0.001114, batch_cost=0.7031, reader_cost=0.41977, ips=5.6895 samples/sec | ETA 02:44:51\n",
      "2021-05-06 21:59:21 [INFO]\t[TRAIN] epoch=13, iter=1940/16000, loss=0.0152, lr=0.001113, batch_cost=0.6574, reader_cost=0.40890, ips=6.0845 samples/sec | ETA 02:34:03\n",
      "2021-05-06 21:59:28 [INFO]\t[TRAIN] epoch=13, iter=1950/16000, loss=0.0134, lr=0.001112, batch_cost=0.6458, reader_cost=0.39677, ips=6.1938 samples/sec | ETA 02:31:13\n",
      "2021-05-06 21:59:34 [INFO]\t[TRAIN] epoch=13, iter=1960/16000, loss=0.0156, lr=0.001111, batch_cost=0.6894, reader_cost=0.44072, ips=5.8019 samples/sec | ETA 02:41:19\n",
      "2021-05-06 21:59:41 [INFO]\t[TRAIN] epoch=13, iter=1970/16000, loss=0.0114, lr=0.001111, batch_cost=0.6992, reader_cost=0.45122, ips=5.7206 samples/sec | ETA 02:43:30\n",
      "2021-05-06 21:59:49 [INFO]\t[TRAIN] epoch=13, iter=1980/16000, loss=0.0165, lr=0.001110, batch_cost=0.7049, reader_cost=0.45721, ips=5.6742 samples/sec | ETA 02:44:43\n",
      "2021-05-06 21:59:55 [INFO]\t[TRAIN] epoch=13, iter=1990/16000, loss=0.0138, lr=0.001109, batch_cost=0.6718, reader_cost=0.42221, ips=5.9540 samples/sec | ETA 02:36:52\n",
      "2021-05-06 22:00:10 [INFO]\t[TRAIN] epoch=13, iter=2010/16000, loss=0.0126, lr=0.001108, batch_cost=0.7077, reader_cost=0.45918, ips=5.6521 samples/sec | ETA 02:45:00\n",
      "2021-05-06 22:00:16 [INFO]\t[TRAIN] epoch=13, iter=2020/16000, loss=0.0151, lr=0.001107, batch_cost=0.6642, reader_cost=0.41530, ips=6.0226 samples/sec | ETA 02:34:45\n",
      "2021-05-06 22:00:23 [INFO]\t[TRAIN] epoch=13, iter=2030/16000, loss=0.0179, lr=0.001106, batch_cost=0.7050, reader_cost=0.45656, ips=5.6740 samples/sec | ETA 02:44:08\n",
      "2021-05-06 22:00:30 [INFO]\t[TRAIN] epoch=13, iter=2040/16000, loss=0.0190, lr=0.001106, batch_cost=0.6702, reader_cost=0.42164, ips=5.9684 samples/sec | ETA 02:35:56\n",
      "2021-05-06 22:00:37 [INFO]\t[TRAIN] epoch=13, iter=2050/16000, loss=0.0229, lr=0.001105, batch_cost=0.6790, reader_cost=0.43041, ips=5.8912 samples/sec | ETA 02:37:51\n",
      "2021-05-06 22:00:44 [INFO]\t[TRAIN] epoch=13, iter=2060/16000, loss=0.0175, lr=0.001104, batch_cost=0.6832, reader_cost=0.43421, ips=5.8545 samples/sec | ETA 02:38:44\n",
      "2021-05-06 22:00:50 [INFO]\t[TRAIN] epoch=13, iter=2070/16000, loss=0.0162, lr=0.001104, batch_cost=0.6637, reader_cost=0.41517, ips=6.0267 samples/sec | ETA 02:34:05\n",
      "2021-05-06 22:00:57 [INFO]\t[TRAIN] epoch=13, iter=2080/16000, loss=0.0106, lr=0.001103, batch_cost=0.7172, reader_cost=0.46893, ips=5.5771 samples/sec | ETA 02:46:23\n",
      "2021-05-06 22:01:04 [INFO]\t[TRAIN] epoch=14, iter=2090/16000, loss=0.0127, lr=0.001102, batch_cost=0.6694, reader_cost=0.42103, ips=5.9754 samples/sec | ETA 02:35:11\n",
      "2021-05-06 22:01:11 [INFO]\t[TRAIN] epoch=14, iter=2100/16000, loss=0.0184, lr=0.001101, batch_cost=0.6954, reader_cost=0.44752, ips=5.7522 samples/sec | ETA 02:41:05\n",
      "2021-05-06 22:01:18 [INFO]\t[TRAIN] epoch=14, iter=2110/16000, loss=0.0266, lr=0.001101, batch_cost=0.6765, reader_cost=0.42887, ips=5.9130 samples/sec | ETA 02:36:36\n",
      "2021-05-06 22:01:25 [INFO]\t[TRAIN] epoch=14, iter=2120/16000, loss=0.0128, lr=0.001100, batch_cost=0.6881, reader_cost=0.43990, ips=5.8133 samples/sec | ETA 02:39:10\n",
      "2021-05-06 22:01:31 [INFO]\t[TRAIN] epoch=14, iter=2130/16000, loss=0.0135, lr=0.001099, batch_cost=0.6704, reader_cost=0.42283, ips=5.9662 samples/sec | ETA 02:34:59\n",
      "2021-05-06 22:01:38 [INFO]\t[TRAIN] epoch=14, iter=2140/16000, loss=0.0125, lr=0.001099, batch_cost=0.6684, reader_cost=0.42045, ips=5.9842 samples/sec | ETA 02:34:24\n",
      "2021-05-06 22:01:45 [INFO]\t[TRAIN] epoch=14, iter=2150/16000, loss=0.0153, lr=0.001098, batch_cost=0.6575, reader_cost=0.40884, ips=6.0841 samples/sec | ETA 02:31:45\n",
      "2021-05-06 22:01:52 [INFO]\t[TRAIN] epoch=14, iter=2160/16000, loss=0.0155, lr=0.001097, batch_cost=0.6969, reader_cost=0.44136, ips=5.7393 samples/sec | ETA 02:40:45\n",
      "2021-05-06 22:01:59 [INFO]\t[TRAIN] epoch=14, iter=2170/16000, loss=0.0127, lr=0.001096, batch_cost=0.6923, reader_cost=0.44251, ips=5.7782 samples/sec | ETA 02:39:33\n",
      "2021-05-06 22:02:05 [INFO]\t[TRAIN] epoch=14, iter=2180/16000, loss=0.0162, lr=0.001096, batch_cost=0.6818, reader_cost=0.43269, ips=5.8670 samples/sec | ETA 02:37:02\n",
      "2021-05-06 22:02:12 [INFO]\t[TRAIN] epoch=14, iter=2190/16000, loss=0.0112, lr=0.001095, batch_cost=0.7035, reader_cost=0.45466, ips=5.6860 samples/sec | ETA 02:41:55\n",
      "2021-05-06 22:02:19 [INFO]\t[TRAIN] epoch=14, iter=2200/16000, loss=0.0128, lr=0.001094, batch_cost=0.6720, reader_cost=0.42358, ips=5.9523 samples/sec | ETA 02:34:33\n",
      "2021-05-06 22:02:26 [INFO]\t[TRAIN] epoch=14, iter=2210/16000, loss=0.0156, lr=0.001094, batch_cost=0.6868, reader_cost=0.43824, ips=5.8239 samples/sec | ETA 02:37:51\n",
      "2021-05-06 22:02:33 [INFO]\t[TRAIN] epoch=14, iter=2220/16000, loss=0.0142, lr=0.001093, batch_cost=0.6630, reader_cost=0.41453, ips=6.0329 samples/sec | ETA 02:32:16\n",
      "2021-05-06 22:02:39 [INFO]\t[TRAIN] epoch=14, iter=2230/16000, loss=0.0163, lr=0.001092, batch_cost=0.6743, reader_cost=0.42657, ips=5.9323 samples/sec | ETA 02:34:44\n",
      "2021-05-06 22:02:46 [INFO]\t[TRAIN] epoch=14, iter=2240/16000, loss=0.0139, lr=0.001091, batch_cost=0.7066, reader_cost=0.45867, ips=5.6608 samples/sec | ETA 02:42:02\n",
      "2021-05-06 22:02:53 [INFO]\t[TRAIN] epoch=15, iter=2250/16000, loss=0.0118, lr=0.001091, batch_cost=0.7002, reader_cost=0.45177, ips=5.7123 samples/sec | ETA 02:40:28\n",
      "2021-05-06 22:03:00 [INFO]\t[TRAIN] epoch=15, iter=2260/16000, loss=0.0157, lr=0.001090, batch_cost=0.6823, reader_cost=0.43447, ips=5.8623 samples/sec | ETA 02:36:15\n",
      "2021-05-06 22:03:07 [INFO]\t[TRAIN] epoch=15, iter=2270/16000, loss=0.0151, lr=0.001089, batch_cost=0.6805, reader_cost=0.43140, ips=5.8781 samples/sec | ETA 02:35:43\n",
      "2021-05-06 22:03:14 [INFO]\t[TRAIN] epoch=15, iter=2280/16000, loss=0.0151, lr=0.001089, batch_cost=0.6603, reader_cost=0.41157, ips=6.0579 samples/sec | ETA 02:30:59\n",
      "2021-05-06 22:03:20 [INFO]\t[TRAIN] epoch=15, iter=2290/16000, loss=0.0121, lr=0.001088, batch_cost=0.6714, reader_cost=0.42269, ips=5.9578 samples/sec | ETA 02:33:24\n",
      "2021-05-06 22:03:27 [INFO]\t[TRAIN] epoch=15, iter=2300/16000, loss=0.0160, lr=0.001087, batch_cost=0.6788, reader_cost=0.43040, ips=5.8929 samples/sec | ETA 02:34:59\n",
      "2021-05-06 22:03:34 [INFO]\t[TRAIN] epoch=15, iter=2310/16000, loss=0.0112, lr=0.001086, batch_cost=0.6695, reader_cost=0.42164, ips=5.9750 samples/sec | ETA 02:32:44\n",
      "2021-05-06 22:03:41 [INFO]\t[TRAIN] epoch=15, iter=2320/16000, loss=0.0117, lr=0.001086, batch_cost=0.6964, reader_cost=0.44788, ips=5.7439 samples/sec | ETA 02:38:46\n",
      "2021-05-06 22:03:47 [INFO]\t[TRAIN] epoch=15, iter=2330/16000, loss=0.0162, lr=0.001085, batch_cost=0.6604, reader_cost=0.41150, ips=6.0573 samples/sec | ETA 02:30:27\n",
      "2021-05-06 22:03:54 [INFO]\t[TRAIN] epoch=15, iter=2340/16000, loss=0.0137, lr=0.001084, batch_cost=0.6917, reader_cost=0.44398, ips=5.7827 samples/sec | ETA 02:37:28\n",
      "2021-05-06 22:04:01 [INFO]\t[TRAIN] epoch=15, iter=2350/16000, loss=0.0140, lr=0.001084, batch_cost=0.6950, reader_cost=0.44730, ips=5.7557 samples/sec | ETA 02:38:06\n",
      "2021-05-06 22:04:08 [INFO]\t[TRAIN] epoch=15, iter=2360/16000, loss=0.0109, lr=0.001083, batch_cost=0.7121, reader_cost=0.46395, ips=5.6172 samples/sec | ETA 02:41:53\n",
      "2021-05-06 22:04:16 [INFO]\t[TRAIN] epoch=15, iter=2370/16000, loss=0.0139, lr=0.001082, batch_cost=0.7106, reader_cost=0.46202, ips=5.6289 samples/sec | ETA 02:41:25\n",
      "2021-05-06 22:04:22 [INFO]\t[TRAIN] epoch=15, iter=2380/16000, loss=0.0169, lr=0.001081, batch_cost=0.6501, reader_cost=0.40097, ips=6.1533 samples/sec | ETA 02:27:33\n",
      "2021-05-06 22:04:29 [INFO]\t[TRAIN] epoch=15, iter=2390/16000, loss=0.0122, lr=0.001081, batch_cost=0.7000, reader_cost=0.45117, ips=5.7144 samples/sec | ETA 02:38:46\n",
      "2021-05-06 22:04:36 [INFO]\t[TRAIN] epoch=15, iter=2400/16000, loss=0.0124, lr=0.001080, batch_cost=0.7127, reader_cost=0.46494, ips=5.6124 samples/sec | ETA 02:41:32\n",
      "2021-05-06 22:04:36 [INFO]\tStart evaluating (total_samples=160, total_iters=160)...\n",
      " 54/160 [=========>....................] - ETA: 12s - batch_cost: 0.1161 - reader cost: 0.102021-05-06 22:05:03 [INFO]\t[TRAIN] epoch=16, iter=2410/16000, loss=0.0122, lr=0.001079, batch_cost=0.7360, reader_cost=0.48476, ips=5.4350 samples/sec | ETA 02:46:41\n",
      "2021-05-06 22:05:10 [INFO]\t[TRAIN] epoch=16, iter=2420/16000, loss=0.0149, lr=0.001079, batch_cost=0.6648, reader_cost=0.41513, ips=6.0168 samples/sec | ETA 02:30:27\n",
      "2021-05-06 22:05:17 [INFO]\t[TRAIN] epoch=16, iter=2430/16000, loss=0.0119, lr=0.001078, batch_cost=0.6970, reader_cost=0.44858, ips=5.7390 samples/sec | ETA 02:37:38\n",
      "2021-05-06 22:05:23 [INFO]\t[TRAIN] epoch=16, iter=2440/16000, loss=0.0139, lr=0.001077, batch_cost=0.6484, reader_cost=0.40006, ips=6.1689 samples/sec | ETA 02:26:32\n",
      "2021-05-06 22:05:30 [INFO]\t[TRAIN] epoch=16, iter=2450/16000, loss=0.0133, lr=0.001076, batch_cost=0.6847, reader_cost=0.43576, ips=5.8419 samples/sec | ETA 02:34:37\n",
      "2021-05-06 22:05:37 [INFO]\t[TRAIN] epoch=16, iter=2460/16000, loss=0.0135, lr=0.001076, batch_cost=0.7019, reader_cost=0.44772, ips=5.6986 samples/sec | ETA 02:38:24\n",
      "2021-05-06 22:05:44 [INFO]\t[TRAIN] epoch=16, iter=2470/16000, loss=0.0173, lr=0.001075, batch_cost=0.6850, reader_cost=0.43672, ips=5.8395 samples/sec | ETA 02:34:27\n",
      "2021-05-06 22:05:51 [INFO]\t[TRAIN] epoch=16, iter=2480/16000, loss=0.0111, lr=0.001074, batch_cost=0.7087, reader_cost=0.45843, ips=5.6445 samples/sec | ETA 02:39:41\n",
      "2021-05-06 22:05:58 [INFO]\t[TRAIN] epoch=16, iter=2490/16000, loss=0.0123, lr=0.001074, batch_cost=0.6889, reader_cost=0.44087, ips=5.8062 samples/sec | ETA 02:35:07\n",
      "2021-05-06 22:06:04 [INFO]\t[TRAIN] epoch=16, iter=2500/16000, loss=0.0149, lr=0.001073, batch_cost=0.6632, reader_cost=0.41464, ips=6.0318 samples/sec | ETA 02:29:12\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "%cd /home/aistudio/PaddleSeg\r\n",
    "!python train.py \\\r\n",
    "       --config configs/unet_PALM.yml \\\r\n",
    "       --do_eval \\\r\n",
    "       --use_vdl \\\r\n",
    "       --save_interval 480 \\\r\n",
    "       --save_dir output_unet_PALMoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n",
      "2021-05-07 18:16:17 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "batch_size: 4\n",
      "iters: 16000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.0\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.00125\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  types:\n",
      "  - coef:\n",
      "    - 0.7\n",
      "    - 0.3\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: DiceLoss\n",
      "    type: MixedLoss\n",
      "model:\n",
      "  num_classes: 2\n",
      "  pretrained: /home/aistudio/unetmodel.pdparams\n",
      "  type: UNet\n",
      "  use_deconv: false\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 4.0e-05\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/work/seg/Train/\n",
      "  mode: train\n",
      "  num_classes: 2\n",
      "  train_path: /home/aistudio/work/seg/Train/train.txt\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 512\n",
      "    - 512\n",
      "    type: Resize\n",
      "  - type: RandomHorizontalFlip\n",
      "  - brightness_range: 0.4\n",
      "    contrast_range: 0.4\n",
      "    saturation_range: 0.4\n",
      "    type: RandomDistort\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/work/seg/Train/\n",
      "  mode: val\n",
      "  num_classes: 2\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 512\n",
      "    - 512\n",
      "    type: Resize\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "  val_path: /home/aistudio/work/seg/Train/val.txt\n",
      "------------------------------------------------\n",
      "W0507 18:16:17.212044  4392 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0507 18:16:17.212100  4392 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n",
      "2021-05-07 18:16:24 [INFO]\tLoading pretrained model from /home/aistudio/unetmodel.pdparams\n",
      "2021-05-07 18:16:24 [WARNING]\t[SKIP] Shape of pretrained params cls.weight doesn't match.(Pretrained: (19, 64, 3, 3), Actual: [2, 64, 3, 3])\n",
      "2021-05-07 18:16:24 [WARNING]\t[SKIP] Shape of pretrained params cls.bias doesn't match.(Pretrained: (19,), Actual: [2])\n",
      "2021-05-07 18:16:24 [WARNING]\t[SKIP] Shape of pretrained params conv.weight doesn't match.(Pretrained: (19, 64, 3, 3), Actual: [2, 64, 3, 3])\n",
      "2021-05-07 18:16:24 [WARNING]\t[SKIP] Shape of pretrained params conv.bias doesn't match.(Pretrained: (19,), Actual: [2])\n",
      "2021-05-07 18:16:25 [INFO]\tThere are 108/112 variables loaded into UNet.\n",
      "2021-05-07 18:16:25 [INFO]\tLoading pretrained model from output_unet_PALMoutput/best_model/model.pdparams\n",
      "2021-05-07 18:16:25 [INFO]\tThere are 112/112 variables loaded into UNet.\n",
      "2021-05-07 18:16:25 [INFO]\tLoaded trained params of model successfully\n",
      "2021-05-07 18:16:25 [INFO]\tStart evaluating (total_samples=160, total_iters=160)...\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py:89: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if isinstance(slot[0], (np.ndarray, np.bool, numbers.Number)):\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.INT32, but right dtype is VarType.BOOL, the right dtype will convert to VarType.INT32\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/math_op_patch.py:238: UserWarning: The dtype of left and right variables are not the same, left dtype is VarType.INT64, but right dtype is VarType.BOOL, the right dtype will convert to VarType.INT64\n",
      "  format(lhs_dtype, rhs_dtype, lhs_dtype))\n",
      "160/160 [==============================] - 26s 163ms/step - batch_cost: 0.1623 - reader cost: 0.147\n",
      "2021-05-07 18:16:51 [INFO]\t[EVAL] #Images=160 mIoU=0.9424 Acc=0.9980 Kappa=0.9390 \n",
      "2021-05-07 18:16:51 [INFO]\t[EVAL] Class IoU: \n",
      "[0.998  0.8868]\n",
      "2021-05-07 18:16:51 [INFO]\t[EVAL] Class Acc: \n",
      "[0.999  0.9366]\n"
     ]
    }
   ],
   "source": [
    "!python val.py --config configs/unet_PALM.yml  --model_path output_unet_PALMoutput/best_model/model.pdparams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n",
      "2021-05-07 18:22:39 [INFO]\t\n",
      "---------------Config Information---------------\n",
      "batch_size: 4\n",
      "iters: 16000\n",
      "learning_rate:\n",
      "  decay:\n",
      "    end_lr: 0.0\n",
      "    power: 0.9\n",
      "    type: poly\n",
      "  value: 0.00125\n",
      "loss:\n",
      "  coef:\n",
      "  - 1\n",
      "  types:\n",
      "  - coef:\n",
      "    - 0.7\n",
      "    - 0.3\n",
      "    losses:\n",
      "    - type: CrossEntropyLoss\n",
      "    - type: DiceLoss\n",
      "    type: MixedLoss\n",
      "model:\n",
      "  num_classes: 2\n",
      "  pretrained: /home/aistudio/unetmodel.pdparams\n",
      "  type: UNet\n",
      "  use_deconv: false\n",
      "optimizer:\n",
      "  momentum: 0.9\n",
      "  type: sgd\n",
      "  weight_decay: 4.0e-05\n",
      "train_dataset:\n",
      "  dataset_root: /home/aistudio/work/seg/Train/\n",
      "  mode: train\n",
      "  num_classes: 2\n",
      "  train_path: /home/aistudio/work/seg/Train/train.txt\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 512\n",
      "    - 512\n",
      "    type: Resize\n",
      "  - type: RandomHorizontalFlip\n",
      "  - brightness_range: 0.4\n",
      "    contrast_range: 0.4\n",
      "    saturation_range: 0.4\n",
      "    type: RandomDistort\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "val_dataset:\n",
      "  dataset_root: /home/aistudio/work/seg/Train/\n",
      "  mode: val\n",
      "  num_classes: 2\n",
      "  transforms:\n",
      "  - target_size:\n",
      "    - 512\n",
      "    - 512\n",
      "    type: Resize\n",
      "  - type: Normalize\n",
      "  type: Dataset\n",
      "  val_path: /home/aistudio/work/seg/Train/val.txt\n",
      "------------------------------------------------\n",
      "W0507 18:22:39.557782  4901 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
      "W0507 18:22:39.557829  4901 device_context.cc:372] device: 0, cuDNN Version: 7.6.\n",
      "2021-05-07 18:22:45 [INFO]\tLoading pretrained model from /home/aistudio/unetmodel.pdparams\n",
      "2021-05-07 18:22:45 [WARNING]\t[SKIP] Shape of pretrained params cls.weight doesn't match.(Pretrained: (19, 64, 3, 3), Actual: [2, 64, 3, 3])\n",
      "2021-05-07 18:22:45 [WARNING]\t[SKIP] Shape of pretrained params cls.bias doesn't match.(Pretrained: (19,), Actual: [2])\n",
      "2021-05-07 18:22:45 [WARNING]\t[SKIP] Shape of pretrained params conv.weight doesn't match.(Pretrained: (19, 64, 3, 3), Actual: [2, 64, 3, 3])\n",
      "2021-05-07 18:22:45 [WARNING]\t[SKIP] Shape of pretrained params conv.bias doesn't match.(Pretrained: (19,), Actual: [2])\n",
      "2021-05-07 18:22:45 [INFO]\tThere are 108/112 variables loaded into UNet.\n",
      "2021-05-07 18:22:45 [INFO]\tNumber of predict images = 400\n",
      "2021-05-07 18:22:45 [INFO]\tLoading pretrained model from output_unet_PALMoutput/best_model/model.pdparams\n",
      "2021-05-07 18:22:46 [INFO]\tThere are 112/112 variables loaded into UNet.\n",
      "2021-05-07 18:22:46 [INFO]\tStart to predict...\n",
      "400/400 [==============================] - 134s 335ms/st\n"
     ]
    }
   ],
   "source": [
    "!python predict.py \\\r\n",
    "       --config configs/unet_PALM.yml \\\r\n",
    "       --model_path output_unet_PALMoutput/best_model/model.pdparams \\\r\n",
    "       --image_path /home/aistudio/work/seg/test \\\r\n",
    "       --save_dir output_unet_PALMoutput/result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 生成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \r\n",
    "import cv2\r\n",
    "result_path = '/home/aistudio/PaddleSeg/output_unet_PALMoutput/result/pseudo_color_prediction'\r\n",
    "dist_path = '/home/aistudio/Disc_Segmentation'\r\n",
    "for img_name in os.listdir(result_path):\r\n",
    "    img_path = os.path.join(result_path, img_name)\r\n",
    "    img = cv2.imread(img_path)\r\n",
    "    g  = img[:,:,1]\r\n",
    "    ret, result = cv2.threshold(g, 127,255, cv2.THRESH_BINARY_INV)\r\n",
    "    cv2.imwrite(os.path.join(dist_path,img_name), result)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 假如预测中出现多个不连通的区域，只保留最大的区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \r\n",
    "import cv2\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "def cnt_area(cnt):\r\n",
    "    area = cv2.contourArea(cnt)\r\n",
    "    return area\r\n",
    "\r\n",
    "result_path = '/home/aistudio/PaddleSeg/output_unet_PALMoutput/result/pseudo_color_prediction'\r\n",
    "dist_path = '/home/aistudio/Disc_Segmentation'\r\n",
    "for img_name in os.listdir(result_path):\r\n",
    "    img_path = os.path.join(result_path, img_name)\r\n",
    "    img = cv2.imread(img_path)\r\n",
    "    g  = img[:,:,1]\r\n",
    "    ret, threshold = cv2.threshold(g, 127,255, cv2.THRESH_BINARY)\r\n",
    "\r\n",
    "\r\n",
    "    contours, hierarch = cv2.findContours(threshold, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\r\n",
    "    contours.sort(key=cnt_area, reverse=True)\r\n",
    "    if len(contours) > 1:\r\n",
    "        for i in range(1,len(contours)):\r\n",
    "            cv2.drawContours(threshold, [contours[i]], 0, 0, -1)\r\n",
    "    _,result = cv2.threshold(threshold, 127, 255, cv2.THRESH_BINARY_INV)\r\n",
    "    cv2.imwrite(os.path.join(dist_path, img_name), result)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 总结\n",
    "模型构建思路及调优过程（可具体包括：思路框架图、思路步骤详述、模型应用+调优过程）\n",
    "\n",
    "【模型】Unet\n",
    "\n",
    "【数据增强】图片大小512x512，水平翻转，对比度随机改变等数据增强\n",
    "\n",
    "【对预测结果进行处理】假如预测中出现多个不连通的区域，只保留最大的区域"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
